{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpype as jp\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEMBEREK_PATH = '...\\\\bin\\\\zemberek-full.jar'\n",
    "\n",
    "# Start the JVM\n",
    "jp.startJVM(\"...\\\\jvm.dll\", \"-ea\", \"-Djava.class.path=%s\" % (ZEMBEREK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('...xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_removed = [sentence.translate(str.maketrans('', '', string.punctuation)) for sentence in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_removed = [re.sub('[0-9]', '', sentences) for sentences in punctuation_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turkishdeasciifier(x):\n",
    "    if \"Ã¶\" in x:\n",
    "        x = x.replace(\"Ã¶\", \"ö\")\n",
    "    if \"Ã¼\" in x:\n",
    "        x= x.replace(\"Ã¼\", \"ü\")\n",
    "    if \"Ã§\" in x:\n",
    "        x= x.replace(\"Ã§\", \"ç\")\n",
    "    if \"ÅŸ\" in x:\n",
    "        x= x.replace(\"ÅŸ\", \"ş\")\n",
    "    if \"Ä±\" in x:\n",
    "        x= x.replace(\"Ä±\", \"ı\")\n",
    "    if \"ÄŸ\" in x:\n",
    "         x= x.replace(\"ÄŸ\", \"ğ\")\n",
    "    if \"Ä°\" in x:\n",
    "        x = x.replace('Ä°', 'İ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"stopwordsturkish.txt\", \"r\")\n",
    "content = file.read()\n",
    "stop_words_list = content.split(\"\\n\")\n",
    "stop_words_list_deasciified = set([turkishdeasciifier(stop_words) for stop_words in stop_words_list if not stop_words == ''])\n",
    "stop_words_removed = [' '.join((set(sentences.split(\" \")) - stop_words_list_deasciified)) for sentences in numbers_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(token):\n",
    "    TurkishMorphology = jp.JClass('zemberek.morphology.TurkishMorphology')\n",
    "    morphology = TurkishMorphology.createWithDefaults()\n",
    "    results = morphology.analyze(token)\n",
    "    investigate_result = str(results)[str(results).index('analysisResults=[')+17:-1]\n",
    "    decided_analysis = investigate_result.split(\", \")[0]\n",
    "    morphology = decided_analysis[decided_analysis.index(\"] \")+2:]\n",
    "    stem = morphology[:morphology.index(\":\")]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "stemmed = []\n",
    "for sent in stop_words_removed:\n",
    "    stemmed_sentence = \"\"\n",
    "    for tokens in tknzr.tokenize(sent):\n",
    "        try:\n",
    "            stemmed_sentence = stemmed_sentence + stemmer(tokens) + \" \"\n",
    "        except:\n",
    "            stemmed_sentence = stemmed_sentence + tokens + \" \"\n",
    "    stemmed.append(stemmed_sentence)\n",
    "print(stemmed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
